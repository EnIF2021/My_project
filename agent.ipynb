{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80f54f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import optimizers as opt\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from config import *\n",
    "from replay_buffer import *\n",
    "from networks import *\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, env, actor_lr=ACTOR_LR, critic_lr=CRITIC_LR, gamma=GAMMA, max_size=BUFFER_CAPACITY, tau=TAU, path_save=PATH_SAVE, path_load=PATH_LOAD):\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.replay_buffer = ReplayBuffer(env, max_size)\n",
    "        self.actions_dim = env.action_space.n\n",
    "        self.actor_lr = actor_lr\n",
    "        self.critic_lr = critic_lr\n",
    "        self.path_save = path_save\n",
    "        self.path_load = path_load\n",
    "        \n",
    "        self.actor = Actor(name='actor', actions_dim=self.actions_dim)\n",
    "        self.critic = Critic(name='critic')\n",
    "        self.target_actor = Actor(name='target_actor', actions_dim=self.actions_dim)\n",
    "        self.target_critic = Critic(name='target_critic')\n",
    "\n",
    "        self.actor.compile(optimizer=opt.Adam(learning_rate=actor_lr))\n",
    "        self.critic.compile(optimizer=opt.Adam(learning_rate=critic_lr))\n",
    "        self.target_actor.compile(optimizer=opt.Adam(learning_rate=actor_lr))\n",
    "        self.target_critic.compile(optimizer=opt.Adam(learning_rate=critic_lr))\n",
    "\n",
    "        actor_weights = self.actor.get_weights()\n",
    "        critic_weights = self.critic.get_weights()\n",
    "        \n",
    "        self.target_actor.set_weights(actor_weights)\n",
    "        self.target_critic.set_weights(critic_weights)\n",
    "        \n",
    "        self.noise = np.zeros(self.actions_dim)\n",
    "        \n",
    "    def update_target_networks(self, tau):\n",
    "        actor_weights = self.actor.weights\n",
    "        target_actor_weights = self.target_actor.weights\n",
    "        for index in range(len(actor_weights)):\n",
    "            target_actor_weights[index] = tau * actor_weights[index] + (1 - tau) * target_actor_weights[index]\n",
    "\n",
    "        self.target_actor.set_weights(target_actor_weights)\n",
    "        \n",
    "        critic_weights = self.critic.weights\n",
    "        target_critic_weights = self.target_critic.weights\n",
    "    \n",
    "        for index in range(len(critic_weights)):\n",
    "            target_critic_weights[index] = tau * critic_weights[index] + (1 - tau) * target_critic_weights[index]\n",
    "\n",
    "        self.target_critic.set_weights(target_critic_weights)\n",
    "    \n",
    "    def add_to_replay_buffer(self, state, action, reward, new_state, done):\n",
    "        self.replay_buffer.add_record(state, action, reward, new_state, done)\n",
    "\n",
    "    def save(self):\n",
    "        date_now = time.strftime(\"%Y%m%d%H%M\")\n",
    "        if not os.path.isdir(f\"{self.path_save}/save_agent_{date_now}\"):\n",
    "            os.makedirs(f\"{self.path_save}/save_agent_{date_now}\")\n",
    "        self.actor.save_weights(f\"{self.path_save}/save_agent_{date_now}/{self.actor.net_name}.h5\")\n",
    "        self.target_actor.save_weights(f\"{self.path_save}/save_agent_{date_now}/{self.target_actor.net_name}.h5\")\n",
    "        self.critic.save_weights(f\"{self.path_save}/save_agent_{date_now}/{self.critic.net_name}.h5\")\n",
    "        self.target_critic.save_weights(f\"{self.path_save}/save_agent_{date_now}/{self.target_critic.net_name}.h5\")\n",
    "        \n",
    "        np.save(f\"{self.path_save}/save_agent_{date_now}/noise.npy\", self.noise)\n",
    "        \n",
    "        self.replay_buffer.save(f\"{self.path_save}/save_agent_{date_now}\")\n",
    "\n",
    "    def load(self):\n",
    "        self.actor.load_weights(f\"{self.path_load}/{self.actor.net_name}.h5\")\n",
    "        self.target_actor.load_weights(f\"{self.path_load}/{self.target_actor.net_name}.h5\")\n",
    "        self.critic.load_weights(f\"{self.path_load}/{self.critic.net_name}.h5\")\n",
    "        self.target_critic.load_weights(f\"{self.path_load}/{self.target_critic.net_name}.h5\")\n",
    "        \n",
    "        self.noise = np.load(f\"{self.path_load}/noise.npy\")\n",
    "        \n",
    "        self.replay_buffer.load(f\"{self.path_load}\")\n",
    "        \n",
    "        \n",
    "        \n",
    "    def _ornstein_uhlenbeck_process(self, x, theta=THETA, mu=0, dt=DT, std=0.2):\n",
    "        \"\"\"\n",
    "        Ornsteinâ€“Uhlenbeck process\n",
    "        \"\"\"\n",
    "        return x + theta * (mu-x) * dt + std * np.sqrt(dt) * np.random.normal(size=self.actions_dim)\n",
    "\n",
    "    def get_action(self, observation, noise, evaluation=False):\n",
    "        state = tf.convert_to_tensor([observation], dtype=tf.float32)\n",
    "        actions = self.actor(state)\n",
    "        if not evaluation:\n",
    "            self.noise = self._ornstein_uhlenbeck_process(noise)\n",
    "            actions += self.noise\n",
    "\n",
    "\n",
    "        return actions.numpy()[0]\n",
    "\n",
    "    def learn(self):\n",
    "        if self.replay_buffer.check_buffer_size() == False:\n",
    "            return\n",
    "\n",
    "        state, action, reward, new_state, done = self.replay_buffer.get_minibatch()\n",
    "\n",
    "        states = tf.convert_to_tensor(state, dtype=tf.float32)\n",
    "        new_states = tf.convert_to_tensor(new_state, dtype=tf.float32)\n",
    "        rewards = tf.convert_to_tensor(reward, dtype=tf.float32)\n",
    "        actions = tf.convert_to_tensor(action, dtype=tf.float32)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            target_actions = self.target_actor(new_states)\n",
    "            target_critic_values = tf.squeeze(self.target_critic(\n",
    "                                new_states, target_actions), 1)\n",
    "            critic_value = tf.squeeze(self.critic(states, actions), 1)\n",
    "            target = reward + self.gamma * target_critic_values * (1-done)\n",
    "            critic_loss = tf.keras.losses.MSE(target, critic_value)\n",
    "\n",
    "        critic_gradient = tape.gradient(critic_loss,\n",
    "                                            self.critic.trainable_variables)\n",
    "        self.critic.optimizer.apply_gradients(zip(\n",
    "            critic_gradient, self.critic.trainable_variables))\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            policy_actions = self.actor(states)\n",
    "            actor_loss = -self.critic(states, policy_actions)\n",
    "            actor_loss = tf.math.reduce_mean(actor_loss)\n",
    "\n",
    "        actor_gradient = tape.gradient(actor_loss, \n",
    "                                    self.actor.trainable_variables)\n",
    "        self.actor.optimizer.apply_gradients(zip(\n",
    "            actor_gradient, self.actor.trainable_variables))\n",
    "\n",
    "        self.update_target_networks(self.tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366100a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
